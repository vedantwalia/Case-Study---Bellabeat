---
title: "Bellabeat Case Study"
output:
  md_document:
    variant: markdown_github

---

# Bellabeat Case Study

## Background Information

**Bellabeat** is a high-tech company that manufactures health-focused smart products. Collecting data on activity, sleep, stress, and reproductive health has allowed Bellabeat to empower women with knowledge about their own health and habits.

### Business Task

Sršen, the company’s cofounder, would like an analysis of Bellabeat’s available consumer data to identify opportunities for growth. She has asked the marketing analytics team to analyze smart device usage data in order to gain insight into how people are already using their smart devices. Then, using this information, she would like recommendations for how these trends can inform Bellabeat marketing strategy. Therefore, in this case study, I will answer the following questions:

1.  What are some trends in smart device usage?
2.  How can these trends help influence Bellabeat marketing strategy?

### About the Data

The data for this case study comes from [Fitbit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit), a public domain dataset available on Kaggle. It contains personal fitness tracker information from 30 FitBit users. These 30 Fitbit users consented to the submission of all personal tracker data contained in this dataset.

I will import the data set from Kaggle into RStudio where I can clean, filter, and analyze the data.

### Limitations

-   Sample size: 30 people is not a large enough sample to be representative of all FitBit users
-   Outdated: The dataset contains data from a one month period in 2016 only. For a deeper and more accurate analysis of trends, we would need data from the current year, preferably collected for an entire year to look at if trends vary during different times of year.
-   Limited: The dataset does not contain any demographic information about the users, including gender, age, or location, which would be beneficial for marketing purposes to target specific customers

## Data Preparation

I can see that there are two folders of data, lets merge the files that have the same name and

```{r}
# Define the paths to the two folders
folder1 <- "/cloud/project/archive/Fitabase Data 3.12.16-4.11.16"
folder2 <- "/cloud/project/archive/Fitabase Data 4.12.16-5.12.16"

# List all files in both folders
files1 <- list.files(folder1, full.names = TRUE)
files2 <- list.files(folder2, full.names = TRUE)

# Get the file names without the path
names1 <- basename(files1)
names2 <- basename(files2)

# Find the common file names
common_files <- intersect(names1, names2)

```

Now I moved all the unique files into the work folder so that further analysis can be done

```{r}
library(dplyr)

# Loop over the common file names
for (file in common_files) {
  # Construct the full paths for both files
  file1_path <- file.path(folder1, file)
  file2_path <- file.path(folder2, file)
  
  # Read the files
  df1 <- read.csv(file1_path)
  df2 <- read.csv(file2_path)
  
  # Merge the data frames row-wise
  merged_df <- bind_rows(df1, df2)
  
  # Save the merged file
  write.csv(merged_df, file.path("/cloud/project/archive", file), row.names = FALSE)
}

# Define the paths to the two folders
folder1 <- "/cloud/project/archive/Fitabase Data 3.12.16-4.11.16"
folder2 <- "/cloud/project/archive/Fitabase Data 4.12.16-5.12.16"
new_folder <- "/cloud/project/archive"

# List all files in both folders
files1 <- list.files(folder1, full.names = TRUE)
files2 <- list.files(folder2, full.names = TRUE)

# Get the file names without the path
names1 <- basename(files1)
names2 <- basename(files2)

# Find the unique file names in folder2
unique_files <- setdiff(names2, names1)

# Loop over the unique file names
for (file in unique_files) {
  # Construct the full path for the file in folder2
  file2_path <- file.path(folder2, file)
  
  # Move the file to the new folder
  file.copy(file2_path, file.path(new_folder, file))
  
  # Optionally, remove the file from folder2
  file.remove(file2_path)
}
```

Now lets start the analysis

## Data Preparation

1. load packages
```{r}
  library(tidyverse)
  library(ggplot2)
  library(lubridate)
  library(lm.beta)
```

2. Load CSV files containing our data

```{r}
  # load files
  daily_activity <- read.csv("./archive/dailyActivity_merged.csv")
  hourly_steps <- read.csv("./archive/hourlySteps_merged.csv")
  daily_sleep <- read.csv("./archive/sleepDay_merged.csv")
  weight <- read.csv("./archive/weightLogInfo_merged.csv")
```

3.  Identify number of participants in each data set by counting distinct IDs
```{r}
  n_distinct(daily_activity$Id)
```
  
```{r}
  n_distinct(hourly_steps$Id)
```
  
```{r}
  n_distinct(daily_sleep$Id)
```

```{r}
  n_distinct(weight$Id)
```

4.  View and clean up the data sets
```{r}
  
  # First, the daily_sleep data
  head(daily_sleep)
  
```

```{r}
  # The 12:00:00 AM time stamp on each observation is redundant so we should remove it to make the  data easier to work with
  daily_sleep$SleepDay <- (gsub('12:00:00 AM', '', daily_sleep$SleepDay))
  # Renaming column
  colnames(daily_sleep)[2] = "Date"
  # View updated data
  head(daily_sleep)
```
```{r}
  # Next, the daily_activity data
  head(daily_activity)
  # The LoggedActivitiesDistance and SedentaryActiveDistance columns don't provide much information
  #   so we will not use them in our analysis and can remove them
  daily_activity <- daily_activity[c(-6, -10)]
  # Renaming column
  colnames(daily_activity)[2] = "Date"
  # View updated Data
  head(daily_activity)
```

```{r}
  # Finally, the hourly_steps data
  head(hourly_steps)
  # In this case, the time associated with the date is relevant so we don't want to remove it,
  #   but the data may be easier to work with if we separate it into it's own column
  hourly_steps <- hourly_steps%>% separate(ActivityHour, c("Date", "Hour"), sep = "^\\S*\\K")
  # View the updated dataframe
  head(hourly_steps)
```
Because the Id variable is currently numerical but should be treated as nominal,we need to change how it is formatted in each data set.
```{r} 
  daily_activity$Id <- as.character(daily_activity$Id)
  daily_sleep$Id <- as.character(daily_sleep$Id)
  hourly_steps$Id <- as.character(hourly_steps$Id)
```
## Data Exploration

1. Graph variables of interest, check for outliers in the data
```{r}
summary(daily_activity$TotalSteps)
ggplot(daily_activity, aes(x = TotalSteps)) +
  geom_boxplot()
# Most of the daily total steps appear to be around 4000-11000.
#   There appear to be possible outliers on the high end

steps_upper <- quantile(daily_activity$TotalSteps, .9985, na.rm = TRUE)
# This shows that 99.85% of the observations are at 28,680 or below. 
# Values above this number are more than 3 standard deviations from the mean, 
#   indicating they are outliers. 

daily_activity <- subset(daily_activity, TotalSteps <= 28680)
# 2 outliers were removed

```

2. Extract more information by running descriptive statistics

#### Sleep Data

* What is the average amount of sleep for each participant?
```{r}
mean_sleep <- daily_sleep %>%
  group_by(Id) %>%
  summarize(mean_sleep = mean(TotalMinutesAsleep)) %>%
  select(Id, mean_sleep) %>%
  arrange(mean_sleep) %>%
  as.data.frame()
head(mean_sleep)
  
```


* What percent of the time did participants actually spend sleeping while laying in bed?
```{r}
daily_sleep %>%
  group_by(Id) %>%
  mutate(percent_sleep = (TotalMinutesAsleep/TotalTimeInBed)*100) %>%
  select(Id, percent_sleep) %>%
  summarize(avg_persleep = mean(percent_sleep)) %>%
  arrange(avg_persleep) %>%
  mutate_if(is.numeric, round, 2)

# Most participants slept for at least 90% of the time they spent in bed, with
#   only 4 participants spending a smaller percent of time sleeping, the lowest
#   being 63.37%
```





